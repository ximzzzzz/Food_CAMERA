{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import easydict\n",
    "sys.path.append('../Whatiswrong')\n",
    "import Extract\n",
    "import utils\n",
    "import torch.nn.functional as F\n",
    "import easydict\n",
    "import torchvision\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-c731985bdd45>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-c731985bdd45>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    self.decoder =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Basemodel(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Basemodel, self).__init__()\n",
    "        self.encoder = Resnet_EFIFSTR(with_lstm=True)\n",
    "        self.decoder = \n",
    "        \n",
    "    def forward(self, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class Resnet_encoder(nn.Module):\n",
    "    def __init__(self, opt, n_group=1):\n",
    "        super(Resnet_encoder, self).__init__()\n",
    "        self.n_group= n_group\n",
    "        self.enc_dim = opt.enc_dim\n",
    "        \n",
    "        in_channels=3\n",
    "        self.layer0 = nn.Sequential(nn.Conv2d(in_channels, 32, kernel_size=(3,3), stride=1, padding=1, bias=False),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.inplanes = 32\n",
    "        self.layer1 = self._make_layer(32, 3, [2,2])  \n",
    "        self.layer2 = self._make_layer(64, 4, [2,2])  \n",
    "        self.layer3 = self._make_layer(128, 6, [2,1]) \n",
    "        self.layer4 = self._make_layer(256, 6, [1,1])\n",
    "        self.layer5 = self._make_layer(512, 3, [1,1])\n",
    "        \n",
    "        self.rnn = nn.LSTM(512, int(self.enc_dim/2), num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.out_planes = 2 * 256\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def _make_layer(self, planes, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride !=[1,1] or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(conv1x1(self.inplanes, planes, stride),\n",
    "                                      nn.BatchNorm2d(planes))\n",
    "            \n",
    "        layers = []\n",
    "        layers.append(ResnetBlock(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResnetBlock(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = self.layer0(x)\n",
    "        x1 = self.layer1(x0)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "        feature_map = x5\n",
    "        feature_map_list = [x1, x2, x3, x4, x5]\n",
    "        \n",
    "        batch_size, channels, feature_h, feature_w = feature_map.shape\n",
    "        cnn_feat = F.max_pool2d(feature_map, (feature_h, 1))\n",
    "        cnn_feat = cnn_feat.permute(0, 3, 1, 2).squeeze(3)\n",
    "\n",
    "        _, holistic_feature = self.rnn(cnn_feat)\n",
    "        \n",
    "        # merge bidirection to uni-direction\n",
    "        hidden_state, cell_state = holistic_feature    #hidden_state (num_layers * num_direction, batch_size, enc_dim / 2 ) \n",
    "        hidden_state = hidden_state.transpose(0,1).contiguous().view(batch_size, -1, self.enc_dim ).transpose(0,1).contiguous()\n",
    "        cell_state = cell_state.transpose(0,1).contiguous().view(batch_size, -1, self.enc_dim).transpose(0,1).contiguous()\n",
    "        \n",
    "        return feature_map_list, (hidden_state, cell_state)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_unit(nn.Module):\n",
    "    \n",
    "    def __init__(self, fmap_dim, lstm_dim, attn_dim):\n",
    "        super(Attention_unit, self).__init__()\n",
    "        self.fmap_dim = fmap_dim\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.e_lstm_conv = nn.Conv2d(lstm_dim, attn_dim, kernel_size=1)\n",
    "        self.e_Fmap_conv = nn.Conv2d(fmap_dim, attn_dim, kernel_size=3, padding=1)\n",
    "        self.a_conv = nn.Conv2d(attn_dim, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, fmap, hidden_state):\n",
    "        batch_size, channel, height, width = fmap.shape\n",
    "        hidden_state = hidden_state.permute(0, 2, 1).unsqueeze(3)\n",
    "        e_lstm_conv_ =  self.e_lstm_conv(hidden_state)\n",
    "        e_lstm_conv_ = e_lstm_conv_.repeat(1, 1, height, width)\n",
    "        e_fmap_conv_ = self.e_Fmap_conv(fmap)\n",
    "#         print('e_lstm_conv res : ', e_lstm_conv_.shape)\n",
    "#         print('e_fmap_conv res : ', e_fmap_conv_.shape)\n",
    "        e = torch.tanh_(e_lstm_conv_ + e_fmap_conv_)\n",
    "        a_conv_ = self.a_conv(e)  \n",
    "        a = F.softmax(a_conv_.reshape((batch_size, -1)), dim = -1)\n",
    "        mask = a.reshape((batch_size, 1, height, width))\n",
    "        broad_casted = (fmap * mask).reshape(batch_size, channel, -1)\n",
    "        glimpse = torch.sum(broad_casted, dim= -1).reshape((batch_size, channel, 1, 1))\n",
    "#         print('glimpse shape : ', glimpse.shape)\n",
    "        return glimpse, mask\n",
    "\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, opt): #att dim = 512\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_classes = opt.num_classes\n",
    "        self.fmap_dim = opt.fmap_dim\n",
    "        self.enc_dim = opt.enc_dim\n",
    "        self.dec_dim = opt.dec_dim\n",
    "        self.attn_dim = opt.attn_dim\n",
    "        self.max_length = opt.max_length\n",
    "        self.attention_unit = Attention_unit(self.fmap_dim, self.dec_dim, self.attn_dim )\n",
    "        self.input_embedding = nn.Embedding(self.num_classes+1, self.enc_dim) # including <BOS>\n",
    "#         self.lstm = nn.LSTMCell(enc_dim, dec_dim)\n",
    "        self.lstm = nn.LSTM(self.enc_dim, self.dec_dim, num_layers = 2, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.dec_dim + self.fmap_dim, self.num_classes +2 ) # including <BOS>,<EOS>\n",
    "        \n",
    "        \n",
    "    def forward(self, feature_map, holistic_feature, Input, is_train):\n",
    "        x, target, length = Input\n",
    "        batch_size, channel, height, width = feature_map.shape\n",
    "\n",
    "        logits = torch.zeros(batch_size, opt.max_length+1, self.num_classes+2)  ### last class : <EOS>\n",
    "        masks = torch.zeros(batch_size, opt.max_length+1, height, width )\n",
    "        glimpses = torch.zeros(batch_size, opt.max_length+1, channel)\n",
    "        input_label = torch.zeros(batch_size, 1, dtype= torch.long).fill_(self.num_classes) #### the second last is used as the <BOS>, \n",
    "\n",
    "        input_emb = self.input_embedding(input_label)\n",
    "        output, states =  self.lstm(input_emb, holistic_feature)\n",
    "        glimpse, mask = self.attention_unit(feature_map, output)\n",
    "        glimpse = glimpse.permute(0, 2, 1, 3).squeeze(3)\n",
    "        logit = self.fc(torch.cat([output, glimpse], axis=2))\n",
    "        logits[:, [0], :] = logit\n",
    "        masks[:,[0], :, : ] = mask\n",
    "        glimpses[:, [0], : ] = glimpse\n",
    "        \n",
    "        if is_train:\n",
    "            for i in range(self.max_length):\n",
    "                input_label = target[:, [i]]\n",
    "                input_emb = self.input_embedding(input_label)\n",
    "                output, states = self.lstm(input_emb, states)\n",
    "                glimpse, mask = self.attention_unit(feature_map, output)\n",
    "                glimpse = glimpse.permute(0, 2, 1, 3).squeeze(3)\n",
    "                logit = self.fc(torch.cat([output, glimpse], axis=2))\n",
    "                logits[:, [i+1], :] = logit\n",
    "                masks[:,[i+1], :, : ] = mask\n",
    "                glimpses[:, [i+1], : ] = glimpse\n",
    "        else:\n",
    "            pred = torch.argmax(logit, dim=-1)\n",
    "            for i in range(1, self.max_length):\n",
    "                input_emb = self.input_embedding(pred)\n",
    "                output, states = self.lstm(input_emb, states)\n",
    "                glimpse, mask = self.attention_unit(feature_map, output)\n",
    "                glimpse = glimpse.permute(0, 2, 1, 3).squeeze(3)\n",
    "                logit = self.fc(torch.cat([output, glimpse], axis=2))\n",
    "                loogits[:, [i], :] = logit \n",
    "                masks[:,[i+1], :, : ] = mask\n",
    "                pred = torch.argmax(torch.softmax(logit, axis=-1), -1)\n",
    "                glimpses[:, [i+1], : ] = glimpse\n",
    "                \n",
    "        return logits, masks, glimpses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc_1 = nn.Linear(int(opt.img_h/2) * int(opt.img_w/2), 16 * 16)\n",
    "        self.fc_2 = nn.Linear(int(opt.img_h/4) * int(opt.img_w/4), 8 * 8)\n",
    "        self.fc_3 = nn.Linear(int(opt.img_h/8) * int(opt.img_w/4), 4 * 4)\n",
    "        self.font_embedding = nn.Embedding(opt.num_fonts, 128)\n",
    "        self.Deconv_1 = nn.ConvTranspose2d(opt.fmap_dim + 128, 128, kernel_size =[2,2], stride=[2,2] )\n",
    "        self.Deconv_2 = nn.ConvTranspose2d(128, 64, kernel_size=[2,2], stride=[2,2]) #kernel size [3,3] make output size  [T, S, 5 , 5]\n",
    "        self.Deconv_3 = nn.ConvTranspose2d((64 + 128), 32, kernel_size=[2,2], stride=[2,2]) # ref : Resnet encoder layer3\n",
    "        self.Deconv_4 = nn.ConvTranspose2d((32 + 64), 16, kernel_size=[2,2], stride=[2,2])  # ref : Resnet encoder layer2\n",
    "        self.Deconv_5 = nn.ConvTranspose2d((16 + 32), 1, kernel_size=[2,2], stride=[2,2])   # ref : Resnet encoder layer1\n",
    "\n",
    "    def generate_glimpse(self, fmap, masks):\n",
    "        _, fmap_c, fmap_h, fmap_w = fmap.shape\n",
    "        mask = F.interpolate(masks, size = (fmap_h, fmap_w), mode='bilinear', align_corners=False)\n",
    "        mask = mask.repeat(1, fmap_c, 1, 1)\n",
    "        fmap = fmap.unsqueeze(1)  #[N, 1, c, 24, 80]\n",
    "        fmap = fmap.repeat(1, self.seq_len, 1, 1, 1).reshape((self.batch_size * self.seq_len, fmap_c, fmap_h, fmap_w))\n",
    "        glimpse = torch.mul(mask, fmap)\n",
    "        \n",
    "        #reshape\n",
    "        glimpse = glimpse.reshape((self.batch_size * self.seq_len, fmap_c, fmap_h * fmap_w))\n",
    "        \n",
    "        return glimpse, fmap_c\n",
    "    \n",
    "    \n",
    "    def forward(self, feature_map_list, masks, glimpses):\n",
    "        \n",
    "        self.batch_size,  self.seq_len,  self.height,  self.width = masks.shape \n",
    "        masks = masks.reshape((self.batch_size * self.seq_len, 1, self.height, self.width))\n",
    "        \n",
    "        glimpse_s1, fmap_s1_c = self.generate_glimpse(feature_map_list[0], masks)  # feature_map_list[0] shape  # 24 * 80\n",
    "        glimpse_s2, fmap_s2_c = self.generate_glimpse(feature_map_list[1], masks)   # 12 * 40\n",
    "        glimpse_s3, fmap_s3_c = self.generate_glimpse(feature_map_list[2], masks)   # 6 * 40\n",
    "        \n",
    "        fmap_last = feature_map_list[-1] # 6 * 40\n",
    "        _, feature_c, feature_h, feature_w = fmap_last.shape       \n",
    "        \n",
    "#         ### fmap s3\n",
    "#         mask_s3 = masks.repeat(1,fmap_s1_c, 1,1)\n",
    "#         fmap_s3.unsqueeze_(1) #  after unsqueeze -> [N, 1, c, 6, 40]\n",
    "#         fmap_s3 = fmap_s3.repeat(1, seq_len, 1, 1, 1).reshape((batch_size * seq_len, fmap_s3_c, fmap_s3_h, fmap_s3_w))\n",
    "#         glimpse_s3 = torch.mul(mask_s3, fmap_s3)\n",
    "        \n",
    "#         ## fmap s2\n",
    "#         mask_s2 = F.interpolate(masks, size = (fmap_s2_h, fmap_s2_w), mode='bilinear', align_corners=False)\n",
    "#         fmap_s2.unsqueeze_(1)  #[N, 1, c, 12, 40]\n",
    "#         fmap_s2 = fmap_s2.repeat(1, seq_len, 1, 1, 1).reshape((batch_size * seq_len, fmap_s2_c, fmap_s2_h, fmap_s2_w))\n",
    "#         glimpse_s2 = torch.mul(mask_s2, fmap_s2)\n",
    "\n",
    "        glimpse_1 = self.fc_1(glimpse_s1).reshape((self.batch_size * self.seq_len , fmap_s1_c, 16, 16))\n",
    "        glimpse_2 = self.fc_2(glimpse_s2).reshape((self.batch_size * self.seq_len , fmap_s2_c, 8, 8))\n",
    "        glimpse_3 = self.fc_3(glimpse_s3).reshape((self.batch_size * self.seq_len , fmap_s3_c, 4, 4))\n",
    "        \n",
    "        embedding_ids = torch.randint(low=0, high= 104, size=(self.batch_size * self.seq_len, 1))\n",
    "        font_embedded = self.font_embedding(embedding_ids).reshape((self.batch_size * self.seq_len, 128, 1, 1))\n",
    "        \n",
    "        # deconv stage\n",
    "        glimpses_deconv = glimpses.reshape((self.batch_size * self.seq_len, feature_c, 1, 1))\n",
    "        concat_deconv = torch.cat([glimpses_deconv, font_embedded], axis=1)\n",
    "        \n",
    "        d1 = self.Deconv_1(concat_deconv)\n",
    "        d2 = self.Deconv_2(d1)\n",
    "        d3 = self.Deconv_3(torch.cat([d2, glimpse_3], dim=1))\n",
    "        d4 = self.Deconv_4(torch.cat([d3, glimpse_2], dim=1))\n",
    "        d5 = self.Deconv_5(torch.cat([d4, glimpse_1], dim=1))\n",
    "        d5 = torch.tanh(d5)\n",
    "        \n",
    "        glyph = d5.reshape((self.batch_size ,self.seq_len, 32 * 32)) \n",
    "        \n",
    "        return glyph, embedding_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = easydict.EasyDict({'max_length' : 10,\n",
    "                        'num_classes' : 10 ,\n",
    "                        'num_fonts' : 104,\n",
    "                        'img_h' : 48,\n",
    "                        'img_w' : 160,\n",
    "                        'fmap_dim' : 512,\n",
    "                        'enc_dim' : 512,\n",
    "                        'attn_dim' : 512,\n",
    "                        'dec_dim' : 512,\n",
    "                        'batch_size' : 10 } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = opt.max_length\n",
    "inputx = torch.LongTensor(list(range(length))*opt.batch_size).reshape((opt.batch_size, length))\n",
    "targetx = torch.LongTensor(list(range(length))*opt.batch_size).reshape((opt.batch_size, length))\n",
    "lengthx = torch.IntTensor([length]*opt.batch_size)\n",
    "x = [inputx, targetx, lengthx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.FloatTensor(opt.batch_size, 3, opt.img_h, opt.img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Resnet_encoder(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_list, holistic_state = encoder(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(opt)\n",
    "logit, masks, glimpses = decoder(fmap_list[-1], holistic_state, x, is_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- glyph_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(opt)\n",
    "glyphs, embedding_ids = generator(fmap_list, masks, glimpses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- recognition loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(0, 10, size = (10,11)) #(10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3972, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(logit, target) #recognition loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_glyphs = torch.Tensor(np.load('data/glyphs_ko_104.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75861],\n",
       "        [ 44046],\n",
       "        [212895],\n",
       "        [137035],\n",
       "        [200662],\n",
       "        [154162],\n",
       "        [ 66077],\n",
       "        [ 70971],\n",
       "        [ 44048],\n",
       "        [ 63630],\n",
       "        [ 14687],\n",
       "        [ 17138],\n",
       "        [127246],\n",
       "        [247150],\n",
       "        [176187],\n",
       "        [119910],\n",
       "        [156608],\n",
       "        [102775],\n",
       "        [227574],\n",
       "        [ 46497],\n",
       "        [188423],\n",
       "        [102780],\n",
       "        [188425],\n",
       "        [107671],\n",
       "        [154161],\n",
       "        [188427],\n",
       "        [161510],\n",
       "        [129699],\n",
       "        [232470],\n",
       "        [193322],\n",
       "        [  7345],\n",
       "        [  4894],\n",
       "        [ 88096],\n",
       "        [ 41602],\n",
       "        [115016],\n",
       "        [ 19576],\n",
       "        [139485],\n",
       "        [ 97887],\n",
       "        [ 29367],\n",
       "        [151715],\n",
       "        [ 36712],\n",
       "        [ 34265],\n",
       "        [171299],\n",
       "        [220232],\n",
       "        [ 34259],\n",
       "        [176192],\n",
       "        [105223],\n",
       "        [146822],\n",
       "        [161511],\n",
       "        [ 92989],\n",
       "        [ 56290],\n",
       "        [ 78306],\n",
       "        [124798],\n",
       "        [ 80751],\n",
       "        [124801],\n",
       "        [110116],\n",
       "        [  2451],\n",
       "        [232467],\n",
       "        [212895],\n",
       "        [ 90546],\n",
       "        [ 39158],\n",
       "        [ 80754],\n",
       "        [ 75862],\n",
       "        [ 31811],\n",
       "        [156610],\n",
       "        [ 92989],\n",
       "        [ 61182],\n",
       "        [ 73416],\n",
       "        [203101],\n",
       "        [249595],\n",
       "        [ 24474],\n",
       "        [203106],\n",
       "        [ 46498],\n",
       "        [230027],\n",
       "        [ 48944],\n",
       "        [159059],\n",
       "        [ 97888],\n",
       "        [ 78304],\n",
       "        [210446],\n",
       "        [244705],\n",
       "        [154162],\n",
       "        [ 68516],\n",
       "        [119910],\n",
       "        [220231],\n",
       "        [244703],\n",
       "        [230026],\n",
       "        [ 53840],\n",
       "        [185972],\n",
       "        [205555],\n",
       "        [227579],\n",
       "        [ 73413],\n",
       "        [183529],\n",
       "        [159056],\n",
       "        [ 68521],\n",
       "        [212889],\n",
       "        [127244],\n",
       "        [102780],\n",
       "        [220230],\n",
       "        [ 22027],\n",
       "        [163949],\n",
       "        [ 95442],\n",
       "        [156610],\n",
       "        [205555],\n",
       "        [129695],\n",
       "        [ 17135],\n",
       "        [ 39155],\n",
       "        [ 36707],\n",
       "        [210449],\n",
       "        [168847],\n",
       "        [110122]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_glyph_ids = target.reshape((110,1)) + 2447* embedding_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_target = ref_glyphs[target_glyph_ids.reshape(-1,)] #reshape 해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- font "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = np.load('./data/glyphs_ko_104.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAkElEQVR4nO3RsQ2CcBDF4R80xoKehIIJSKAgBliA0gEcgN7FjG5iYmhItKAjamFhY3wOwP2Vyopr78vLu5wnvo//Yz8NJGHzgE2wbi0hvZewk3LYajw+XJ6QQQcrO2EPkTQAvZ1wggpaiCO75BkK6KByXHGFEgY3uLFI4e4ASIeslnSs45fRUd5/fjGDGUwGHylkYIPSY1HiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x7F1114B97780>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to_img(((fonts[2447*100 + 3].reshape((32, 32, 1))+1.0) * 127.5).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
