{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import easydict\n",
    "sys.path.append('../Whatiswrong')\n",
    "import Extract\n",
    "import utils\n",
    "import torch.nn.functional as F\n",
    "import easydict\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basemodel(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Basemodel, self).__init__()\n",
    "        self.encoder = Resnet_EFIFSTR(with_lstm=True)\n",
    "        self.decoder = \n",
    "        \n",
    "    def forward(self, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class Resnet_encoder(nn.Module):\n",
    "    def __init__(self, n_group=1):\n",
    "        super(Resnet_encoder, self).__init__()\n",
    "        self.n_group= n_group\n",
    "        \n",
    "        in_channels=3\n",
    "        self.layer0 = nn.Sequential(nn.Conv2d(in_channels, 32, kernel_size=(3,3), stride=1, padding=1, bias=False),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.inplanes = 32\n",
    "        self.layer1 = self._make_layer(32, 3, [2,2])\n",
    "        self.layer2 = self._make_layer(64, 4, [2,2])\n",
    "        self.layer3 = self._make_layer(128, 6, [2,1])\n",
    "        self.layer4 = self._make_layer(256, 6, [1,1])\n",
    "        self.layer5 = self._make_layer(512, 3, [1,1])\n",
    "        \n",
    "        self.rnn = nn.LSTM(512, int(512/2), num_layers=2, batch_first=True)\n",
    "        self.out_planes = 2 * 256\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def _make_layer(self, planes, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride !=[1,1] or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(conv1x1(self.inplanes, planes, stride),\n",
    "                                      nn.BatchNorm2d(planes))\n",
    "            \n",
    "        layers = []\n",
    "        layers.append(ResnetBlock(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResnetBlock(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = self.layer0(x)\n",
    "        x1 = self.layer1(x0)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "        feature_map = x5\n",
    "        feature_map_list = [x1, x2, x3, x4, x5]\n",
    "        \n",
    "        batch_size, channels, feature_h, feature_w = feature_map.shape\n",
    "        cnn_feat = F.max_pool2d(feature_map, (feature_h, 1))\n",
    "        cnn_feat = cnn_feat.permute(0, 3, 1, 2).squeeze(3)\n",
    "\n",
    "        _, holistic_feature = self.rnn(cnn_feat)\n",
    "        return feature_map_list, holistic_feature\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, num_classes, fmap_dim, enc_dim, dec_dim, att_dim, opt): #att dim = 512\n",
    "        super(Decoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.attention_unit = Attention_unit(fmap_dim, dec_dim, att_dim )\n",
    "        self.input_embedding = nn.Embedding(self.opt.num_classes+1, enc_dim) # including <BOS>\n",
    "#         self.lstm = nn.LSTMCell(enc_dim, dec_dim)\n",
    "        self.lstm = nn.LSTM(enc_dim, dec_dim, num_layers = 2, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(dec_dim + fmap_dim, self.opt.num_classes +2 ) # including <BOS>,<EOS>\n",
    "        \n",
    "        \n",
    "    def forward(self, feature_map, holistic_feature, Input, is_train):\n",
    "        x, target, length = Input\n",
    "        batch_size, channel, height, width = feature_map.shape\n",
    "\n",
    "        logits = torch.zeros(batch_size, opt.max_length+1, opt.num_classes+2).fill_(opt.num_classes) ### last class : <EOS>\n",
    "        masks = torch.zeros(batch_size, opt.max_length+1, height, width )\n",
    "        input_label = torch.zeros(batch_size, 1, dtype= torch.long).fill_(self.opt.num_classes) #### the second last is used as the <BOS>, \n",
    "\n",
    "        input_emb = self.input_embedding(input_label)\n",
    "        output, states =  self.lstm(input_emb, holistic_feature)\n",
    "        glimpse, mask = self.attention_unit(feature_map, output)\n",
    "        glimpse = glimpse.permute(0, 2, 1, 3).squeeze(3)\n",
    "        logit = self.fc(torch.cat([output, glimpse], axis=2))\n",
    "        logits[:, [0], :] = logit\n",
    "        masks[:,[0], :, : ] = mask\n",
    "        \n",
    "        if is_train:\n",
    "            for i in range(self.opt.max_length):\n",
    "                input_label = target[:, [i]]\n",
    "                input_emb = self.input_embedding(input_label)\n",
    "                output, states = self.lstm(input_emb, states)\n",
    "                glimpse, mask = self.attention_unit(feature_map, output)\n",
    "                glimpse = glimpse.permute(0, 2, 1, 3).squeeze(3)\n",
    "                logit = self.fc(torch.cat([output, glimpse], axis=2))\n",
    "                logits[:, [i+1], :] = logit\n",
    "                masks[:,[i+1], :, : ] = mask\n",
    "                \n",
    "        else:\n",
    "            pred = torch.argmax(logit, dim=-1)\n",
    "            for i in range(1, self.opt.max_length):\n",
    "                input_emb = self.input_embedding(pred)\n",
    "                output, states = self.lstm(input_emb, states)\n",
    "                glimpse, mask = self.attention_unit(feature_map, output)\n",
    "                glimpse = glimpse.permute(0, 2, 1, 3).squeeze(3)\n",
    "                logit = self.fc(torch.cat([output, glimpse], axis=2))\n",
    "                loogits[:, [i], :] = logit \n",
    "                masks[:,[i+1], :, : ] = mask\n",
    "                pred = torch.argmax(torch.softmax(logit, axis=-1), -1)\n",
    "        \n",
    "        return logits, masks\n",
    "        \n",
    "        \n",
    "class Attention_unit(nn.Module):\n",
    "    \n",
    "    def __init__(self, fmap_dim, lstm_dim, attn_dim):\n",
    "        super(Attention_unit, self).__init__()\n",
    "        self.fmap_dim = fmap_dim\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.e_lstm_conv = nn.Conv2d(lstm_dim, attn_dim, kernel_size=1)\n",
    "        self.e_Fmap_conv = nn.Conv2d(fmap_dim, attn_dim, kernel_size=3, padding=1)\n",
    "        self.a_conv = nn.Conv2d(attn_dim, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, fmap, hidden_state):\n",
    "        batch_size, channel, height, width = fmap.shape\n",
    "        hidden_state = hidden_state.permute(0, 2, 1).unsqueeze(3)\n",
    "        e_lstm_conv_ =  self.e_lstm_conv(hidden_state)\n",
    "        e_lstm_conv_ = e_lstm_conv_.repeat(1, 1, height, width)\n",
    "        e_fmap_conv_ = self.e_Fmap_conv(fmap)\n",
    "#         print('e_lstm_conv res : ', e_lstm_conv_.shape)\n",
    "#         print('e_fmap_conv res : ', e_fmap_conv_.shape)\n",
    "        e = torch.tanh_(e_lstm_conv_ + e_fmap_conv_)\n",
    "        a_conv_ = self.a_conv(e)  \n",
    "        a = F.softmax(a_conv_.reshape((batch_size, -1)), dim = -1)\n",
    "        mask = a.reshape((batch_size, 1, height, width))\n",
    "        broad_casted = (fmap * mask).reshape(batch_size, channel, -1)\n",
    "        glimpse = torch.sum(broad_casted, dim= -1).reshape((batch_size, channel, 1, 1))\n",
    "#         print('glimpse shape : ', glimpse.shape)\n",
    "        return glimpse, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator_1 = nn.Linear(int(opt.img_h/2) * int(opt.img_w/2), 16 * 16)\n",
    "        self.generator_2 = nn.Linear(int(opt.img_h/4) * int(opt.img_w/4), 8 * 8)\n",
    "        self.generator_3 = nn.Linear(int(opt.img_h/8) * int(opt.img_w/4), 4 * 4)\n",
    "        self.font_embedding = nn.Embedding(opt.num_fonts, 128)\n",
    "\n",
    "    def generate_glimpse(self, fmap_s1, masks):\n",
    "        _, fmap_s1_c, fmap_s1_h, fmap_s1_w = fmap_s1.shape\n",
    "        mask_s1 = F.interpolate(masks, size = (fmap_s1_h, fmap_s1_w), mode='bilinear', align_corners=False)\n",
    "        mask_s1 = mask_s1.repeat(1, fmap_s1_c, 1, 1)\n",
    "        fmap_s1.unsqueeze_(1)  #[N, 1, c, 24, 80]\n",
    "        fmap_s1 = fmap_s1.repeat(1, self.seq_len, 1, 1, 1).reshape((self.batch_size * self.seq_len, fmap_s1_c, fmap_s1_h, fmap_s1_w))\n",
    "        glimpse_s1 = torch.mul(mask_s1, fmap_s1)\n",
    "        \n",
    "        #reshape\n",
    "        glimpse_s1 = glimpse_s1.reshape((self.batch_size * self.seq_len, fmap_s1_c, fmap_s1_h * fmap_s1,w))\n",
    "        \n",
    "        return glimpse_s1, fmap_s1_c\n",
    "    \n",
    "    \n",
    "    def forward(self, feature_map_list, masks):\n",
    "        \n",
    "        self.batch_size,  self.seq_len,  self.height,  self.width = masks.shape \n",
    "        masks = masks.reshape((batch_size * seq_len, 1, height, width))\n",
    "        \n",
    "        glimpse_s1, fmap_s1_c = generate_glimpse(feature_map_list[0], masks)  # feature_map_list[0] shape  # 24 * 80\n",
    "        glimpse_s2, fmap_s2_c = generate_glimpse(feature_map_list[1], masks)   # 12 * 40\n",
    "        glimpse_s3, fmap_s3_c = generate_glimpse(feature_map_list[2], masks)   # 6 * 40\n",
    "        _, feature_c, feature_h, feature_w = fmap_last.shape       \n",
    "\n",
    "        fmap_last = feature_map_list[-1] # 6 * 40\n",
    "        \n",
    "#         ### fmap s3\n",
    "#         mask_s3 = masks.repeat(1,fmap_s1_c, 1,1)\n",
    "#         fmap_s3.unsqueeze_(1) #  after unsqueeze -> [N, 1, c, 6, 40]\n",
    "#         fmap_s3 = fmap_s3.repeat(1, seq_len, 1, 1, 1).reshape((batch_size * seq_len, fmap_s3_c, fmap_s3_h, fmap_s3_w))\n",
    "#         glimpse_s3 = torch.mul(mask_s3, fmap_s3)\n",
    "        \n",
    "#         ## fmap s2\n",
    "#         mask_s2 = F.interpolate(masks, size = (fmap_s2_h, fmap_s2_w), mode='bilinear', align_corners=False)\n",
    "#         fmap_s2.unsqueeze_(1)  #[N, 1, c, 12, 40]\n",
    "#         fmap_s2 = fmap_s2.repeat(1, seq_len, 1, 1, 1).reshape((batch_size * seq_len, fmap_s2_c, fmap_s2_h, fmap_s2_w))\n",
    "#         glimpse_s2 = torch.mul(mask_s2, fmap_s2)\n",
    "\n",
    "        gen_1 = self.generator_1(glimpse_s1)\n",
    "        gen_2 = self.generator_2(glimpse_s2)\n",
    "        gen_3 = self.generator_3(glimpse_s3)\n",
    "        \n",
    "        gen_1 = gen_1.reshape((self.batch_size * self.seq_len , fmap_s1_c, 16, 16))\n",
    "        gen_2 = gen_2.reshape((self.batch_size * self.seq_len , fmap_s2_c, 8, 8))\n",
    "        gen_3 = gen_3.reshape((self.batch_size * self.seq_len , fmap_s1_c, 4, 4))\n",
    "        \n",
    "        embedding_ids = torch.randint(low=0, high= 104, size=(self.batch_size * self.seq_len))\n",
    "        font_embedded = self.font_embedding(embedding_ids)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.compat.v1.get_variable(\"Embedding\", [325, 1, 1, 128], tf.float32, tf.random_normal_initializer(stddev=0.01), trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ids = tf.random.uniform([2], minval = 0, maxval = 325, dtype= tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tf.gather(embeddings , embeddings_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1, 128])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(104, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(0, 104, size = (20,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_res = emb(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5062, -1.0473, -0.6756,  ...,  0.1441, -0.1520,  0.4853]]],\n",
       "\n",
       "\n",
       "        [[[-1.1960, -0.1352, -1.1072,  ..., -0.4492, -0.8839, -0.7102]]],\n",
       "\n",
       "\n",
       "        [[[-1.1994,  0.1184,  0.8018,  ...,  1.3211, -1.6681,  2.1872]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.3782,  0.5842,  0.2902,  ..., -0.0369, -0.5360,  1.3972]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3536,  1.3173, -1.6219,  ...,  1.2961, -0.8625, -0.2497]]],\n",
       "\n",
       "\n",
       "        [[[-1.8092,  0.2794, -0.2372,  ..., -1.5593,  0.9193,  0.6989]]]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_res.reshape((20, 1, 1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = easydict.EasyDict({'max_length' : 10,\n",
    "                        'num_classes' : 10 ,\n",
    "                        'num_fonts' : 104})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse = torch.mul(torch.FloatTensor(16, 3, 20, 20).uniform_(-1,1), torch.FloatTensor(16,3,20,20).uniform_(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse_ = glimpse.reshape((16,3, 20*20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nn.Linear(20*20, 16*16)(glimpse_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 256])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "length = opt.max_length\n",
    "inputx = torch.LongTensor(list(range(length))*batch_size).reshape((batch_size, length))\n",
    "targetx = torch.LongTensor(list(range(length))*batch_size).reshape((batch_size, length))\n",
    "lengthx = torch.IntTensor([length]*batch_size)\n",
    "x = [inputx, targetx, lengthx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = torch.FloatTensor(batch_size, 512, 65, 65).uniform_(-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "holistic_feature = [torch.FloatTensor(2, batch_size, 64).uniform_(-1.0, 1.0), torch.FloatTensor(2, batch_size, 64).uniform_(-1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(num_classes=10, fmap_dim = 512, enc_dim= 32, dec_dim= 64, att_dim=128, opt = opt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logit, masks = decoder(feature_map, holistic_feature, x, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 11, 65, 65])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Resnet_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.FloatTensor(1, 3, 48, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_list, holistic = encoder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 12, 40])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmap_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 256])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holistic[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 256])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holistic[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(512, 256, num_layers = 2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = torch.FloatTensor(1,35,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_input = lstm_input[:, :1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hiddens = lstm(one_input, holistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-961ed67232b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 256])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 512])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens[0].transpose(0,1).contiguous().view(1, -1, 512).transpose(0,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- font "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = np.load('./data/glyphs_ko_104.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAkElEQVR4nO3RsQ2CcBDF4R80xoKehIIJSKAgBliA0gEcgN7FjG5iYmhItKAjamFhY3wOwP2Vyopr78vLu5wnvo//Yz8NJGHzgE2wbi0hvZewk3LYajw+XJ6QQQcrO2EPkTQAvZ1wggpaiCO75BkK6KByXHGFEgY3uLFI4e4ASIeslnSs45fRUd5/fjGDGUwGHylkYIPSY1HiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x7F1114B97780>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to_img(((fonts[2447*100 + 3].reshape((32, 32, 1))+1.0) * 127.5).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
