{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import gc\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "import json\n",
    "import functools\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.5, 0.5, 0.5]\n",
    "STD = [0.5, 0.5, 0.5]\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 8\n",
    "EPOCH = 1\n",
    "TQDM_DISABLE = True\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(paths):\n",
    "    all_images = []\n",
    "    for path in paths:\n",
    "        image_df = pd.read_parquet(path)\n",
    "        images = image_df.iloc[:, 1:].values.reshape(-1, 137, 236).astype(np.uint8)\n",
    "        del image_df\n",
    "        gc.collect()\n",
    "        all_images.append(images)\n",
    "    all_images = np.concatenate(all_images)\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "font_data = pd.read_csv('font.csv')\n",
    "font_images = load_images(['font_image_data_0.parquet',\n",
    "                          'font_image_data_1.parquet',\n",
    "                          'font_image_data_2.parquet',\n",
    "                          'font_image_data_3.parquet'])\n",
    "np.save('font_images.npy', font_images)\n",
    "del font_images\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "multi_diacritics_train_data = pd.read_csv('train_multi_diacritics.csv')\n",
    "train_data = train_data = train_data.set_index('image_id')\n",
    "multi_diacritics_train_data = multi_diacritics_train_data.set_index('image_id')\n",
    "train_data.update(multi_diacritics_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = load_images(['train_image_data_0.parquet',\n",
    "                           'train_image_data_1.parquet',\n",
    "                           'train_image_data_2.parquet',\n",
    "                           'train_image_data_3.parquet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_images = np.load('font_images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GraphemeDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, images, transform=None, num_grapheme_root=168, num_vowel_diacritic=11, num_consonant_diacritic=8):\n",
    "        self.data = data\n",
    "        self.grapheme_root_list = np.array(data['grapheme_root'].tolist(), dtype=np.int64)\n",
    "        self.vowel_diacritic_list = np.array(data['vowel_diacritic'].tolist(), dtype=np.int64)\n",
    "        self.consonant_diacritic_list = np.array(data['consonant_diacritic'].tolist(), dtype=np.int64)\n",
    "        self.num_grapheme_root = num_grapheme_root\n",
    "        self.num_vowel_diacritic = num_vowel_diacritic\n",
    "        self.num_consonant_diacritic = num_consonant_diacritic\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        grapheme_root = self.grapheme_root_list[idx]\n",
    "        vowel_diacritic = self.vowel_diacritic_list[idx]\n",
    "        consonant_diacritic = self.consonant_diacritic_list[idx]\n",
    "        label = (grapheme_root*self.num_vowel_diacritic+vowel_diacritic)*self.num_consonant_diacritic+consonant_diacritic\n",
    "        np_image = self.images[idx].copy()\n",
    "        out_image = self.transform(np_image)\n",
    "        return out_image, label\n",
    "    \n",
    "    \n",
    "class Albumentations:\n",
    "    def __init__(self, augmentations):\n",
    "        self.augmentations = A.Compose(augmentations)\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = self.augmentations(image=image)['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = [\n",
    "    A.CenterCrop(height=137, width=IMG_WIDTH),\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n",
    "]\n",
    "\n",
    "augmentations = [\n",
    "    A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], always_apply=True),\n",
    "    A.imgaug.transforms.IAAAffine(shear=5, mode='constant', cval=255, always_apply=True),\n",
    "    A.ShiftScaleRotate(rotate_limit=5, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], mask_value=[255, 255, 255], always_apply=True),\n",
    "    A.RandomCrop(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n",
    "]\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    np.uint8,\n",
    "    transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n",
    "    np.uint8,\n",
    "    Albumentations(preprocess + augmentations),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "#     transforms.ToPILImage(),\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    np.uint8,\n",
    "    transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n",
    "    np.uint8,\n",
    "    Albumentations(preprocess),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "#     transforms.ToPILImage(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_dataset = GraphemeDataset(train_data, train_images, valid_transform)\n",
    "font_dataset = GraphemeDataset(font_data, font_images, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_block=6, padding_type='reflect'):\n",
    "        assert(n_blocks >=0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "            \n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "        \n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2 , kernel_size= 3, stride=2, padding=1, bias=use_bias),\n",
    "                     norm_layer(ngf * mult * 2),\n",
    "                     nn.ReLU(True)]\n",
    "            \n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model +=[ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer = norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "            \n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias),\n",
    "                     norm_layer(int(ngf * mult / 2)),\n",
    "                     nn.ReLU(True)]\n",
    "            model += [nn.ReflectionPad2d(3)]\n",
    "            model += [nn.Conv2d(ngf, output_nc, kernel_size =7, padding=0)]\n",
    "            model += [nn.Tanh()]\n",
    "            \n",
    "            self.model = nn.Sequential(*model)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "        \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        conv_block = []\n",
    "        \n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type =='replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type =='zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "            \n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias = use_bias), norm_layer(dim), nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block +=[nn.Dropout(0.5)]\n",
    "            \n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer = nn.BatchNorm2d):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        \n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size = kw, stride=2, padding=padw), \n",
    "                    nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in rnage(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            sequence += [nn.Conv2d(ndf * nf_mult_prev, ndf*nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                         norm_layer(ndf * nf_mult), \n",
    "                         nn.LeakyReLU(0.2, True)]\n",
    "            \n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size = kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        \n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(net, init_gain):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear')!= -1):\n",
    "            init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            if hasattr(m, 'bias'):\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "    net.apply(init_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0 :\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "            \n",
    "    def query(self, images):\n",
    "        if self.pool_size ==0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images:\n",
    "            image = torch.unsqueeze(image.data, 0)\n",
    "            if self.num_imgs < self.pool_size :\n",
    "                self.num_imgs = self.num_imgs +1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0,1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size - 1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "                    \n",
    "        return_images = torch.cat(return_images, 0) \n",
    "        return return_images\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode, target_real_label = 1.0, target_fake_label = 0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode in ['wganp']:\n",
    "            self.loss = None\n",
    "        else :\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "            \n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        \n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "    \n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        \n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengalModel(nn.Module):\n",
    "    def __init__(self, backbone, hidden_size = 2560, class_num = 168*11*7):\n",
    "        super(BengalModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self._avg_pooling = nn.AdaptiveAvgPool2d(1) #square shape\n",
    "        self.fc = nn.Linear(hidden_size, class_num)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bs = inputs.shape[0]\n",
    "        feature = self.backbone.extract_features(inputs)\n",
    "        feature_vector = self._avg_pooling(feature)\n",
    "        feature_vector = feature_vector.view(bs, -1)\n",
    "        feature_vector = self.ln(feature_vector)\n",
    "        \n",
    "        out = self.fc(feature_vector)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
