{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import *\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import easydict\n",
    "import sys\n",
    "sys.path.append('./Whatiswrong')\n",
    "sys.path.append('./Scatter')\n",
    "\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import utils\n",
    "from utils import *\n",
    "import augs\n",
    "import www_model_jamo\n",
    "import torch.distributed as dist\n",
    "# from apex.parallel import DistributedDataParallel as DDP\n",
    "import en_dataset\n",
    "import ko_dataset\n",
    "from albumentations import GaussNoise, IAAAdditiveGaussianNoise, Compose, OneOf\n",
    "from albumentations.pytorch import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from './Whatiswrong/utils.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt\n",
    "opt = easydict.EasyDict({\n",
    "    \"experiment_name\" : f'{utils.SaveDir_maker(base_model = \"www_jamo\", base_model_dir = \"./models\")}',\n",
    "    'saved_model' : '',\n",
    "    \"manualSeed\" : 1111,\n",
    "    \"imgH\" : 35 ,\n",
    "    \"imgW\" :  90,\n",
    "    \"PAD\" : True ,\n",
    "    'batch_size' : 128,\n",
    "    'data_filtering_off' : True,\n",
    "    'workers' : 20,\n",
    "    'rgb' :True,\n",
    "    'sensitive' : True,\n",
    "    'top_char' : ' !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~01234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㄲㄸㅃㅆㅉ',\n",
    "    'middle_char' : ' ㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣㅐㅒㅔㅖㅘㅙㅚㅝㅞㅟㅢ',\n",
    "    'bottom_char' : ' ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㄲㄸㅃㅆㅉㄳㄵㄶㄺㄻㄼㄽㄾㄿㅀㅄ',\n",
    "    'batch_max_length' : 25,\n",
    "    'num_fiducial' : 20,\n",
    "    'output_channel' : 512,\n",
    "    'hidden_size' :256,\n",
    "    'lr' : 1,\n",
    "    'rho' : 0.95,\n",
    "    'eps' : 1e-8,\n",
    "    'grad_clip' : 5,\n",
    "    'valInterval' : 200,\n",
    "    'num_epoch' : 100,\n",
    "    'input_channel' : 3,\n",
    "    'FT' : True,\n",
    "    'extract' : 'RCNN'\n",
    "    })\n",
    "\n",
    "top_converter = utils.AttnLabelConverter(opt.top_char)\n",
    "middle_converter = utils.AttnLabelConverter(opt.middle_char)\n",
    "bottom_converter = utils.AttnLabelConverter(opt.bottom_char)\n",
    "opt.top_n_cls = len(top_converter.character)\n",
    "opt.middle_n_cls = len(middle_converter.character)\n",
    "opt.bottom_n_cls = len(bottom_converter.character)\n",
    "device = torch.device('cuda') #utils.py 안에 device는 따로 세팅해줘야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:09<00:00, 20277.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Seed setting \"\"\"\n",
    "# print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "np.random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "torch.cuda.manual_seed(opt.manualSeed)\n",
    "\n",
    "# KOREAN\n",
    "data=[]\n",
    "# ko_hand = ko_dataset.hand_dataset(num_samples = 100000, dataset_mode = 'word', label_mode = 'jamo') #순서병신\n",
    "# ko_public = ko_dataset.public_crop(mode = 'jamo') #분리 병신\n",
    "ko_synthetic = ko_dataset.korean_synthetic(need_samples = 200000, mode='jamo')\n",
    "\n",
    "# ENGLISH \n",
    "# eng_dataset = en_dataset.get_english_dataset(mode='jamo')\n",
    "# eng_synthetic = en_dataset.en_synthetic(mode='jamo', need_samples=2000000) #0 for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data :  200000\n"
     ]
    }
   ],
   "source": [
    "# data.extend(ko_hand.dataset)\n",
    "# data.extend(ko_public.dataset)\n",
    "data.extend(ko_synthetic.dataset)\n",
    "# data.extend(eng_dataset)\n",
    "# data.extend(eng_synthetic.dataset)\n",
    "random.shuffle(data)\n",
    "print('Total number of data : ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = Compose([\n",
    "                        OneOf([\n",
    "#                                   augs.VinylShining(1),\n",
    "                            augs.GridMask(num_grid=(10,20)),\n",
    "                            augs.RandomAugMix(severity=1, width=1)], p =0.4),\n",
    "                            ToTensor()\n",
    "                       ])\n",
    "train_custom = utils.CustomDataset_jamo(data[ : int(len(data) * 0.95)], resize_shape = (opt.imgH, opt.imgW), transformer=transformers)\n",
    "valid_custom = utils.CustomDataset_jamo(data[ int(len(data) * 0.95): ], resize_shape = (opt.imgH, opt.imgW), transformer=ToTensor())\n",
    "\n",
    "data_loader = DataLoader(train_custom, batch_size = opt.batch_size,  num_workers =15, shuffle=True, drop_last=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_custom, batch_size = opt.batch_size,  num_workers=10, shuffle=True,  drop_last=True, pin_memory=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Criterion(input, target, size_average=True):\n",
    "#     \"\"\"Categorical cross-entropy with logits input and one-hot target\"\"\"\n",
    "#     l = -(target * torch.log(F.softmax(input, dim=1) + 1e-10)).sum(1)\n",
    "#     if size_average:\n",
    "#         l = l.mean()\n",
    "#     else:\n",
    "#         l = l.sum()\n",
    "#     return l\n",
    "\n",
    "def train(opt):\n",
    "    model = www_model_jamo.STR(opt, device)\n",
    "    print('model parameters. height {}, width {}, num of fiducial {}, input channel {}, output channel {}, hidden size {}, \\\n",
    "    batch max length {}'.format(opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel, opt.hidden_size, opt.batch_max_length))\n",
    "    \n",
    "    # weight initialization\n",
    "    for name, param, in model.named_parameters():\n",
    "        if 'localization_fc2' in name:\n",
    "            print(f'Skip {name} as it is already initializaed')\n",
    "            continue\n",
    "        try:\n",
    "            if 'bias' in name:\n",
    "                init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                init.kaiming_normal_(param)\n",
    "                \n",
    "        except Exception as e :\n",
    "            if 'weight' in name:\n",
    "                param.data.fill_(1)\n",
    "            continue\n",
    "\n",
    "    if opt.saved_model != '':\n",
    "        base_path = './models'\n",
    "        print(f'looking for pretrained model from {os.path.join(base_path, opt.saved_model)}')\n",
    "        \n",
    "        try :\n",
    "            model.load_state_dict(torch.load(os.path.join(base_path, opt.saved_model)))\n",
    "            print('loading complete ')    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('coud not find model')\n",
    "            \n",
    "    #data parallel for multi GPU\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "    model.train() \n",
    "     \n",
    "    # loss\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device) #ignore [GO] token = ignore index 0\n",
    "    log_avg = Averager()\n",
    "    \n",
    "    # filter that only require gradient descent\n",
    "    filtered_parameters = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p : p.requires_grad, model.parameters()):\n",
    "        filtered_parameters.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Tranable params : ', sum(params_num))\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = optim.Adadelta(filtered_parameters, lr= opt.lr, rho = opt.rho, eps = opt.eps)\n",
    "#     optimizer = adabound.AdaBound(filtered_parameters, lr=1e-3, final_lr=0.1)\n",
    "    \n",
    "    # opt log\n",
    "    with open(f'./models/{opt.experiment_name}/opt.txt', 'a') as opt_file:\n",
    "        opt_log = '---------------------Options-----------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log +=f'{str(k)} : {str(v)}\\n'\n",
    "        opt_log +='---------------------------------------------\\n'\n",
    "        opt_file.write(opt_log)\n",
    "        \n",
    "    #start training\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "\n",
    "    for n_epoch, epoch in enumerate(range(opt.num_epoch)):\n",
    "        for n_iter, data_point in enumerate(data_loader):\n",
    "#             image_tensors, top, mid, bot = data_point \n",
    "# #             print(f'top : {top}')\n",
    "# #             print(f'mid : {mid}')\n",
    "# #             print(f'bot : {bot}')\n",
    "#             image = image_tensors.to(device)\n",
    "#             text_top, length_top = top_converter.encode(top, batch_max_length = opt.batch_max_length)\n",
    "#             text_mid, length_mid = middle_converter.encode(mid, batch_max_length = opt.batch_max_length)\n",
    "#             text_bot, length_bot = bottom_converter.encode(bot, batch_max_length = opt.batch_max_length)\n",
    "#             batch_size = image.size(0)\n",
    "            \n",
    "# #             onehot = torch.FloatTensor(batch_size, opt.batch_max_length+2, len(opt.character)+2).zero_().to(device)\n",
    "# #             text_onehot = onehot.scatter(dim = 2, index = text.unsqueeze(2).to(device), value = 1 ) #(bs, batch_max_length, num_characters)\n",
    "                \n",
    "#             pred_top, pred_mid, pred_bot = model(image, text_top[:,:-1], text_mid[:,:-1], text_bot[:,:-1])\n",
    "# #             preds = model(image, text_onehot[:, : -1])\n",
    "# #             target = text_onehot[:, 1:]\n",
    "#             cost_top = criterion(pred_top.view(-1, pred_top.shape[-1]), text_top[:, 1:].contiguous().view(-1))\n",
    "#             cost_mid = criterion(pred_mid.view(-1, pred_mid.shape[-1]), text_mid[:, 1:].contiguous().view(-1))\n",
    "#             cost_bot = criterion(pred_bot.view(-1, pred_bot.shape[-1]), text_bot[:, 1:].contiguous().view(-1))\n",
    "# #             cost = Criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1, target.shape[-1]))\n",
    "#             cost = cost_top + cost_mid + cost_bot\n",
    "    \n",
    "#             loss_avg = Averager()\n",
    "\n",
    "#             model.zero_grad()\n",
    "#             cost.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) #gradient clipping with 5\n",
    "#             optimizer.step()\n",
    "\n",
    "#             loss_avg.add(cost)\n",
    "\n",
    "            #validation\n",
    "            if (n_iter % opt.valInterval == 0) & (n_iter!=0):\n",
    "                elapsed_time = time.time() - start_time\n",
    "                with open(f'./models/{opt.experiment_name}/log_train.txt', 'a') as log:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        valid_loss, current_accuracy, current_norm_ED, pred_top_str, pred_mid_str, pred_bot_str, label_top, label_mid, label_bot, infer_time, length_of_data = utils.validation_jamo(model, criterion, valid_loader, top_converter, middle_converter, bottom_converter, opt)\n",
    "                    model.train()\n",
    "\n",
    "                    present_time = time.localtime()\n",
    "                    loss_log = f'[epoch : {n_epoch}/{opt.num_epoch}] [iter : {n_iter*opt.batch_size} / {int(len(data) * 0.95)}]\\n'+\\\n",
    "                    f'Train loss : {loss_avg.val():0.5f}, Valid loss : {valid_loss:0.5f}, Elapsed time : {elapsed_time:0.5f}, Present time : {present_time[1]}/{present_time[2]}, {present_time[3]+9} : {present_time[4]}'\n",
    "                    loss_avg.reset()\n",
    "\n",
    "                    current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"current_norm_ED\":17s}: {current_norm_ED:0.2f}'\n",
    "\n",
    "                    #keep the best\n",
    "                    if current_accuracy > best_accuracy:\n",
    "                        best_accuracy = current_accuracy\n",
    "                        torch.save(model.module.state_dict(), f'./models/{opt.experiment_name}/best_accuracy.pth')\n",
    "\n",
    "                    if current_norm_ED > best_norm_ED:\n",
    "                        best_norm_ED = current_norm_ED\n",
    "                        torch.save(model.module.state_dict(), f'./models/{opt.experiment_name}/best_norm_ED.pth')\n",
    "\n",
    "                    best_model_log = f'{\"Best accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.2f}'\n",
    "                    loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                    print(loss_model_log)\n",
    "                    log.write(loss_model_log+'\\n')\n",
    "\n",
    "                    dashed_line = '-'*80\n",
    "                    head = f'{\"Ground Truth\":25s} | {\"Prediction\" :25s}| T/F'\n",
    "                    predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "\n",
    "                    random_idx  = np.random.choice(range(len(labels)), size= 5, replace=False)\n",
    "                    for gt, pred in zip(list(np.asarray([label_top, label_mid, label_bot])[:, random_idx]), list(np.asarray([pred_top_str, pred_mid_str, pred_bot_str])[:, random_idx])):\n",
    "#                         gt = gt[: gt.find('[s]')]\n",
    "#                         pred = pred[: pred.find('[s]')]\n",
    "                        print(f'gt : {gt}')\n",
    "                        print(f'pred : {pred}')\n",
    "                        predicted_result_log += f'{gt:25s} | {pred:25s} | \\t{str(pred == gt)}\\n'\n",
    "                    predicted_result_log += f'{dashed_line}'\n",
    "                    print(predicted_result_log)\n",
    "                    log.write(predicted_result_log+'\\n')\n",
    "\n",
    "        if (n_epoch) % 5 ==0:\n",
    "            torch.save(model.module.state_dict(), f'./models/{opt.experiment_name}/{n_epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters. height 35, width 90, num of fiducial 20, input channel 3, output channel 512, hidden size 256,     batch max length 25\n",
      "Skip Trans.LocalizationNetwork.localization_fc2.weight as it is already initializaed\n",
      "Skip Trans.LocalizationNetwork.localization_fc2.bias as it is already initializaed\n",
      "Tranable params :  8643606\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ㅐ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d548721444e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     opt.batch_size = opt.batch_size * opt.num_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-9d00044e20fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_norm_ED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_top_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_mid_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bot_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_bot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_of_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_jamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_converter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiddle_converter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom_converter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/FoodDetection/AI_OCR/Whatiswrong/utils.py\u001b[0m in \u001b[0;36mvalidation_jamo\u001b[0;34m(model, criterion, evaluation_loader, top_converter, middle_converter, bottom_converter, opt)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mlength_for_pred_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_max_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mtext_for_pred_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_max_length\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_n_cls\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mtext_for_loss_mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_for_loss_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mlength_for_pred_bot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_max_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/FoodDetection/AI_OCR/Whatiswrong/utils.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, batch_max_length)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# +1 for [s]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mbatch_max_length\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;31m# additional +1 for [GO] at first step. batch_text is padded with [GO]  token after [s] token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mbatch_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_max_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Data/FoodDetection/AI_OCR/Whatiswrong/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# +1 for [s]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mbatch_max_length\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;31m# additional +1 for [GO] at first step. batch_text is padded with [GO]  token after [s] token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mbatch_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_max_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ㅐ'"
     ]
    }
   ],
   "source": [
    "os.makedirs(f'./models/{opt.experiment_name}', exist_ok=True)\n",
    "\n",
    "# set seed\n",
    "random.seed(opt.manualSeed)\n",
    "np.random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "torch.cuda.manual_seed(opt.manualSeed)\n",
    "\n",
    "# set GPU\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "opt.num_gpu = torch.cuda.device_count()\n",
    "\n",
    "# if opt.num_gpu > 1:\n",
    "#     print('-------- Use multi GPU setting --------')\n",
    "#     opt.workers = opt.workers * opt.num_gpu\n",
    "#     opt.batch_size = opt.batch_size * opt.num_gpu\n",
    "\n",
    "train(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#먼저실행함\n",
    "from jamo import h2j, j2hcj, j2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j2hcj(h2j('회'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation by visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()\n",
    "font_location = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_location)\n",
    "device = torch.device('cpu') \n",
    "\n",
    "class Valid_visualizer():\n",
    "    def __init__(self, opt, model_path, val_path, visual_samples, device):\n",
    "        self.opt = opt\n",
    "        self.model_path = model_path\n",
    "        self.val_path = val_path\n",
    "        self.dataset = self._get_dataset()\n",
    "        self.visual_samples = visual_samples\n",
    "        self.device = device\n",
    "        \n",
    "    def _load_model(self):\n",
    "        model = www_model.STR(self.opt, self.device)\n",
    "        model.load_state_dict(torch.load(self.model_path))\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "    def _get_dataset(self):\n",
    "        val_list = os.listdir(self.val_path)\n",
    "        val_dataset = []\n",
    "        label = 'ㄱ'\n",
    "        for val in val_list:\n",
    "#             img = Image.open(f'./val/{val}').convert('RGB')\n",
    "            val_dataset.append([os.path.join(self.val_path, val), label])\n",
    "        return val_dataset\n",
    "    \n",
    "    def _get_valid_loader(self):\n",
    "\n",
    "        test_streamer = utils.Dataset_streamer(self.dataset, resize_shape = (opt.imgH, opt.imgW), transformer=ToTensor())\n",
    "#         _AlignCollate = utils.AlignCollate(imgH=self.opt.imgH, imgW=self.opt.imgW, keep_ratio_with_pad=True)\n",
    "#         test_loader = DataLoader(test_streamer, batch_size = len(self.dataset), num_workers =0, collate_fn = _AlignCollate,)\n",
    "        test_loader = DataLoader(test_streamer, batch_size = self.visual_samples, num_workers =0)\n",
    "        return iter(test_loader)\n",
    "    \n",
    "    \n",
    "    def valid_visualize(self):\n",
    "        random.shuffle(self.dataset)\n",
    "        test_loader_iter = self._get_valid_loader()\n",
    "        image_tensor, label = next(test_loader_iter)\n",
    "        model = self._load_model()\n",
    "        output = model(input = image_tensor, text= ' ', is_train=False)\n",
    "        pred_index = output.max(2)[1]\n",
    "        pred_length = torch.IntTensor([opt.batch_max_length] * self.visual_samples).to(device)\n",
    "        pred_decode = converter.decode(pred_index, pred_length)\n",
    "        preds = []\n",
    "        \n",
    "        for pred in pred_decode:\n",
    "            pred_temp = pred[ : pred.find('[s]')]\n",
    "        #             pred_temp = join_jamos(pred_temp)\n",
    "            preds.append(pred_temp)\n",
    "        \n",
    "        n_cols = 5\n",
    "        n_rows = int(np.ceil(self.visual_samples/n_cols))\n",
    "        last = self.visual_samples % n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols)\n",
    "        fig.set_size_inches((30, 30))\n",
    "        i=0      \n",
    "        for row in range(n_rows):\n",
    "            for col in range(n_cols):\n",
    "                axes[row][col].imshow(Image.open(self.dataset[i][0]))\n",
    "                axes[row][col].set_xlabel(f'Prediction : {preds[i]}', fontproperties=fontprop, fontsize=30)\n",
    "                i+=1\n",
    "                if (row==n_rows-1) & (col==last-1):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = Valid_visualizer(opt, model_path = './models/www_0708/2/best_accuracy_91.602.pth', val_path = './val', visual_samples = 8, device= device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vv.valid_visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
