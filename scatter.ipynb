{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import *\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import easydict\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from jamo import h2j, j2hcj, j2h\n",
    "\n",
    "sys.path.append('./Whatiswrong')\n",
    "sys.path.append('./Scatter')\n",
    "import scatter_utils\n",
    "import utils\n",
    "import Trans\n",
    "import Extract\n",
    "import VFR\n",
    "import SCR\n",
    "import CTC\n",
    "import en_dataset\n",
    "import ko_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Data/FoodDetection/AI_OCR/utils.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt\n",
    "opt = easydict.EasyDict({\n",
    "    \"experiment_name\" : 'scatter_0609_adadelta',\n",
    "    'saved_model' : 'scatter_0609_adadelta/best_accuracy.pth',\n",
    "    \"imgH\" : 35 ,\"imgW\" :  90,  'batch_size' : 384, \n",
    "    'character' : '0123456789ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎㄵㄶㄺㄻㅀㄼㅄabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣㅐㅒㅔㅖㅢㅟㅝㅞㅚㅘㅙ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' ,\n",
    "    'batch_max_length' : 25,\n",
    "    'output_channel' : 512, 'hidden_size' :256,\n",
    "    'valinterval' : 200, 'num_epoch' : 300, 'input_channel' : 3,\n",
    "    'n_scrb' : 2, 'scr_loss_lambda' : 1, 'ctc_loss_lambda' : 0.1,\n",
    "    'lr' : 1, 'rho' : 0.95, 'eps' : 1e-8,\n",
    "    'grad_clip' : 5,\n",
    "    \"manualSeed\" : 1111, \"PAD\" : True ,'data_filtering_off' : True,'rgb' :True,'sensitive' : True, 'FT' : True,\n",
    "    'num_fiducial' : 20,\n",
    "    })\n",
    "converter = utils.AttnLabelConverter(opt.character)\n",
    "opt.num_classes = len(converter.character)\n",
    "device = torch.device('cuda') #utils.py 안에 device는 따로 세팅해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:06<00:00, 16518.20it/s]\n"
     ]
    }
   ],
   "source": [
    "ko_dataset = ko_dataset.dataset(num_samples = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOREAN SYNTHETIC\n",
    "kor_path = '/Data/FoodDetection/data/text_recognition/Korean/synthetic_data/data'\n",
    "\n",
    "kor_images_labels = []\n",
    "with open(os.path.join(kor_path, 'gt.txt'), 'r') as f:\n",
    "    files = f.readlines()\n",
    "    for idx, file in enumerate(files):\n",
    "        img_path, label = file.split(' ')\n",
    "        label = j2hcj(h2j(label.strip('\\n')))\n",
    "        img = Image.open(os.path.join(kor_path, f'{img_path}.jpg'))\n",
    "        kor_images_labels.append([img, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot identify image file '/Data/FoodDetection/data/text_recognition/English/synthetic_oxford2/77_heretical_35885.jpg'\n"
     ]
    }
   ],
   "source": [
    "# ENGLISH STR\n",
    "\n",
    "# eng_dataset = en_dataset.get_english_dataset(opt) # 얘는 일단보류\n",
    "eng_path = '/Data/FoodDetection/data/text_recognition/English/synthetic_oxford2'\n",
    "file_list = os.listdir(eng_path)\n",
    "eng_images_labels = []\n",
    "for file in file_list[:300000]:\n",
    "    try :\n",
    "        img = Image.open(os.path.join(eng_path, file))\n",
    "        label = file.split('_')[1].strip('\\n')\n",
    "        eng_images_labels.append([img, label])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_dataset.dataset.extend(kor_images_labels)\n",
    "ko_dataset.dataset.extend(eng_images_labels)\n",
    "random.shuffle(ko_dataset.dataset)\n",
    "len(ko_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = utils.get_transform()\n",
    "dataset_streamer = utils.Dataset_streamer(ko_dataset.dataset[ : int(len(ko_dataset.dataset)*0.95)],\n",
    "                                          transformer=transform\n",
    "                                         )\n",
    "valid_streamer = utils.Dataset_streamer(ko_dataset.dataset[int(len(ko_dataset.dataset)*0.95) : ], \n",
    "                                        transformer=transform\n",
    "                                       )\n",
    "\n",
    "_AlignCollate = utils.AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=False)\n",
    "data_loader = DataLoader(dataset_streamer, batch_size = opt.batch_size,  num_workers =0, shuffle=True, #worker_init_fn=worker_init_fn, \n",
    "                         collate_fn = _AlignCollate, pin_memory=False )\n",
    "data_loader_iter = iter(data_loader)\n",
    "\n",
    "#for valid\n",
    "_AlignCollate_valid = utils.AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=True)\n",
    "#not valid_streamer\n",
    "valid_loader = DataLoader(valid_streamer, batch_size = opt.batch_size,  num_workers=0, shuffle=True, #worker_init_fn = worker_init_fn,\n",
    "                          collate_fn=_AlignCollate_valid, pin_memory=False)\n",
    "valid_loader_iter = iter(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(data_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx :  132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'ㅈㅣㄷㅏ')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = np.random.choice(range(opt.batch_size), size=1)[0]\n",
    "plt.imshow(img[random_idx].numpy().transpose().swapaxes(1,0))\n",
    "print('idx : ',random_idx)\n",
    "plt.xlabel(label[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, length = converter.encode(label, batch_max_length = opt.batch_max_length)\n",
    "img_tensor = img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCATTER(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(SCATTER, self).__init__()\n",
    "        self.opt = opt\n",
    "        \n",
    "        #Trans\n",
    "        self.Trans = Trans.TPS_SpatialTransformerNetwork(F = opt.num_fiducial, i_size = (opt.imgH, opt.imgW), \n",
    "                                                  i_r_size= (opt.imgH, opt.imgW), i_channel_num=opt.input_channel, device = device)\n",
    "        \n",
    "        #Extract\n",
    "        self.Extract = Extract.RCNN_extractor(opt.input_channel, opt.output_channel)\n",
    "#         self.Extract = Extract.ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        self.FeatureExtraction_output = opt.output_channel # (imgH/16 -1 )* 512\n",
    "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None,1)) # imgH/16-1   ->  1\n",
    "            \n",
    "        # VISUAL FEATURES \n",
    "        self.VFR = VFR.Visual_Features_Refinement(kernel_size = (3,1), num_classes = opt.num_classes, \n",
    "                                              in_channels = self.FeatureExtraction_output, out_channels=1, stride=1)\n",
    "        \n",
    "        # CTC DECODER\n",
    "        self.CTC = CTC.CTC_decoder(opt.output_channel, opt.output_channel, opt.num_classes, opt, device)\n",
    "            \n",
    "        # Selective Contextual Refinement Block\n",
    "        self.SCR_1 = SCR.Selective_Contextual_refinement_block(input_size = self.FeatureExtraction_output, \n",
    "                                                         hidden_size = int(self.FeatureExtraction_output/2),\n",
    "                                                        output_size = self.FeatureExtraction_output,\n",
    "                                                        num_classes = opt.num_classes, decoder_fix = False, device = device)\n",
    "        \n",
    "        self.SCR_2 = SCR.Selective_Contextual_refinement_block(input_size = self.FeatureExtraction_output, \n",
    "                                                         hidden_size = int(self.FeatureExtraction_output/2),\n",
    "                                                        output_size = self.FeatureExtraction_output,\n",
    "                                                        num_classes = opt.num_classes, decoder_fix = True, device = device)\n",
    "  \n",
    "    def forward(self, input, text, is_train=True):\n",
    "        # Trans stage\n",
    "        input = self.Trans(input)\n",
    "        \n",
    "        # Extract stage\n",
    "        visual_feature = self.Extract(input) # visual_feature.shape) # (192, 512, 1 , 23)\n",
    "        \n",
    "        # Visual Feature Refinement\n",
    "        visual_refined = self.VFR(visual_feature) # visual_ refined output Size([192, 23, 512])\n",
    "        \n",
    "        # CTC DECODER\n",
    "        ctc_prob  = self.CTC(visual_refined, text, opt)\n",
    "        \n",
    "        #Selective Contextual Refinement\n",
    "        scr_probs_1, H = self.SCR_1(visual_feature.permute(0, 3, 1, 2).squeeze(3), text, is_train)\n",
    "        scr_probs_2, _ = self.SCR_2(H, text, is_train)\n",
    "        \n",
    "        return [scr_probs_1, scr_probs_2] ,ctc_prob\n",
    "#         return [scr_probs_1, scr_probs_2] ,_\n",
    "#         return _, ctc_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter = SCATTER(opt)\n",
    "# scatter = scatter.cuda()\n",
    "# scr_probs, ctc_prob = scatter(img_tensor, text[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(scr_probs))\n",
    "# print(ctc_prob.shape)\n",
    "# ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 23, 143])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 27])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_criterion = torch.nn.CTCLoss(blank= 0, reduction = 'mean', zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_lengths = torch.full(size = (ctc_prob.size(0),), fill_value= ctc_prob.size(1), dtype=torch.long)\n",
    "output_lengths = torch.randint(low = 1, high = ctc_prob.size(1), size = (ctc_prob.size(0), ), dtype = torch.long)\n",
    "ctc_loss = ctc_criterion(ctc_prob.transpose(0,1), text[:, 1:], input_lengths, output_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opt):\n",
    "    \n",
    "    model = SCATTER(opt)\n",
    "    \n",
    "    # WEIGHT INITIALIZATION\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if 'localization_fc2' in name:\n",
    "#             print(f'Skip {name} as it is already initializaed')\n",
    "#             continue\n",
    "            \n",
    "#         try:\n",
    "#             if 'bias' in name:\n",
    "#                 init.constant_(param, 0.0)\n",
    "#             elif 'weight' in name:\n",
    "#                 init.kaiming_normal_(parm)\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             if 'weight' in name:\n",
    "#                 param.data.fill_(1)\n",
    "#             continue\n",
    "    \n",
    "#     print('layer SCR_1 initial weight :', model.SCR_1.BiLSTM_1.weight_ih_l0)\n",
    "#     print('layer SCR_1 initial grad :', model.SCR_1.BiLSTM_1.weight_ih_l0.grad)\n",
    "    \n",
    "    # DATA PARALLEL\n",
    "    model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)\n",
    "#     model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # CHECK PRETRAINED MODEL\n",
    "    if opt.saved_model!='':\n",
    "        base_path = './models'\n",
    "        try:\n",
    "            if opt.FT:\n",
    "                model.load_state_dict(torch.load(os.path.join(base_path, opt.saved_model)), strict=False)\n",
    "            else :\n",
    "                model.load_state_dict(torch.load(os.path.join(base_path, opt.saved_model)))\n",
    "            print('got your model!')\n",
    "        except Exception as e:\n",
    "            print(f'Coudnt load model. error is {e}')\n",
    "    \n",
    "    # LOSS \n",
    "    scr_criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "    ctc_criterion = torch.nn.CTCLoss(blank= 0, reduction = 'mean', zero_infinity=True).to(device)\n",
    "    \n",
    "    # Filter only require gradient descent\n",
    "    filtered_params = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p : p.requires_grad, model.parameters()):\n",
    "        filtered_params.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Trainable params : ', sum(params_num))\n",
    "    \n",
    "    # OPTIMIZER\n",
    "    optimizer = optim.Adadelta(filtered_params, lr= opt.lr, rho = opt.rho, eps = opt.eps)\n",
    "#     optimizer = optim.Adam(filtered_params)\n",
    "\n",
    "    # OPT LOG\n",
    "    with open(f'./models/{opt.experiment_name}/opt.txt', 'a') as opt_file:\n",
    "        opt_log = '-----------------Options--------------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log +=f'{str(k)} : {str(v)}\\n'\n",
    "        opt_log+= '--------------------------------------------\\n' \n",
    "        opt_file.write(opt_log)\n",
    "        \n",
    "    # START TRAIN\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    \n",
    "    for n_epoch, epoch in enumerate(range(opt.num_epoch)):\n",
    "        try :\n",
    "            for n_iter, data_point in enumerate(data_loader):\n",
    "                images, labels = data_point\n",
    "                images = images.to(device)\n",
    "                text, length = converter.encode(labels, batch_max_length = opt.batch_max_length)\n",
    "\n",
    "                batch_size = images.size(0)\n",
    "\n",
    "                scr_probs, ctc_prob = model(images, text[:, :-1])\n",
    "                target = text[:, 1:]\n",
    "\n",
    "                losses = []\n",
    "                input_lengths = torch.full(size = (ctc_prob.size(0),), fill_value= ctc_prob.size(1), dtype=torch.long)\n",
    "                output_lengths = torch.randint(low = 1, high = ctc_prob.size(1), size = (ctc_prob.size(0), ), dtype = torch.long)\n",
    "                ctc_loss = ctc_criterion(ctc_prob.transpose(0,1), text[:, 1:], input_lengths, output_lengths)\n",
    "\n",
    "                losses.append(opt.ctc_loss_lambda * ctc_loss)\n",
    "                for prob in scr_probs:\n",
    "                    scr_loss = scr_criterion(prob.view(-1, prob.shape[-1]), target.contiguous().view(-1))\n",
    "                    losses.append(opt.scr_loss_lambda * scr_loss)\n",
    "\n",
    "                loss=0\n",
    "                loss_avg = utils.Averager()\n",
    "                for loss_ in losses:\n",
    "                    loss+=loss_\n",
    "                    loss_avg.add(loss_)\n",
    "\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                #VALIDATION\n",
    "                if (n_iter % opt.valinterval ==0) & (n_iter!=0):\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    with open(f'./models/{opt.experiment_name}/log_train.txt' , 'a') as log:\n",
    "                        model.eval()\n",
    "                        with torch.no_grad():  #criterion arguments = ctc_criterion first\n",
    "                            valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels, infer_time, length_of_data = scatter_utils.validation(model, \n",
    "                            [ctc_criterion, scr_criterion], valid_loader, converter, opt)  \n",
    "                        model.train()\n",
    "\n",
    "                        present_time = time.localtime()\n",
    "                        loss_log = f'[epoch : {n_epoch}/{opt.num_epoch}] [iter : {n_iter*opt.batch_size} / {int(len(ko_dataset.dataset)*0.98)}]\\n'+\\\n",
    "                        f'Train loss : {loss_avg.val():0.5f}, Valid loss : {valid_loss:0.5f}, Elapsed time : {elapsed_time:0.5f}, Present time : {present_time[1]}/{present_time[2]}, {present_time[3]} : {present_time[4]}'\n",
    "                        loss_avg.reset()\n",
    "\n",
    "                        current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"current_norm_ED\":17s}: {current_norm_ED:0.2f}'\n",
    "\n",
    "                        #keep the best\n",
    "                        if current_accuracy > best_accuracy:\n",
    "                            best_accuracy = current_accuracy\n",
    "                            torch.save(model.state_dict(), f'./models/{opt.experiment_name}/best_accuracy.pth')\n",
    "\n",
    "                        if current_norm_ED > best_norm_ED:\n",
    "                            best_norm_ED = current_norm_ED\n",
    "                            torch.save(model.state_dict(), f'./models/{opt.experiment_name}/best_norm_ED.pth')\n",
    "\n",
    "                        best_model_log = f'{\"Best accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.2f}'\n",
    "                        loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                        print(loss_model_log)\n",
    "                        log.write(loss_model_log+'\\n')\n",
    "\n",
    "                        dashed_line = '-'*80\n",
    "                        head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s}|Confidence Score & T/F'\n",
    "                        predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "\n",
    "                        for gt, pred, confidence in zip(list(np.asarray(labels)[:5]), list(np.asarray(preds)[:5]), list(np.asarray(confidence_score)[:5])):\n",
    "\n",
    "                            gt = gt[: gt.find('[s]')]\n",
    "                            pred = pred[: pred.find('[s]')]\n",
    "\n",
    "                            predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                        predicted_result_log += f'{dashed_line}'\n",
    "                        print(predicted_result_log)\n",
    "                        log.write(predicted_result_log+'\\n')\n",
    "\n",
    "            if n_epoch % 10 ==0:\n",
    "                torch.save(model.state_dict(), f'./models/{opt.experiment_name}/{n_epoch}.pth')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Use multi GPU setting-------\n",
      "got your model!\n",
      "Trainable params :  37193125\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f'./models/{opt.experiment_name}', exist_ok=True)\n",
    "\n",
    "# set seed\n",
    "random.seed(opt.manualSeed)\n",
    "np.random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "torch.cuda.manual_seed(opt.manualSeed)\n",
    "\n",
    "# set GPU\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "opt.num_gpu = torch.cuda.device_count()\n",
    "\n",
    "if opt.num_gpu > 1:\n",
    "    print('-------Use multi GPU setting-------')\n",
    "    opt.batch_size = opt.batch_size * opt.num_gpu\n",
    "train(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
