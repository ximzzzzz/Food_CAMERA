{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import *\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import easydict\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from jamo import h2j, j2hcj, j2h\n",
    "\n",
    "sys.path.append('./Whatiswrong')\n",
    "sys.path.append('./Scatter')\n",
    "import scatter_utils\n",
    "import utils\n",
    "import Trans\n",
    "import Extract\n",
    "import VFR\n",
    "import SCR\n",
    "import CTC\n",
    "import en_dataset\n",
    "import ko_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scatter_utils' from './Scatter/scatter_utils.py'>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(scatter_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt\n",
    "opt = easydict.EasyDict({\n",
    "    \"experiment_name\" : 'scatter_0608',\n",
    "    'saved_model' : 'scatter_0608/best_accuracy.pth',\n",
    "    \"imgH\" : 35 ,\"imgW\" :  90,  'batch_size' : 64, \n",
    "    'character' : '0123456789ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎㄵㄶㄺㄻㅀㄼㅄabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣㅐㅒㅔㅖㅢㅟㅝㅞㅚㅘㅙ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' ,\n",
    "    'batch_max_length' : 25,\n",
    "    'output_channel' : 512, 'hidden_size' :256,\n",
    "    'valinterval' : 100, 'num_epoch' : 300, 'input_channel' : 3,\n",
    "    'n_scrb' : 2, 'scr_loss_lambda' : 1, 'ctc_loss_lambda' : 0.1,\n",
    "    'lr' : 1, 'rho' : 0.95, 'eps' : 1e-8,\n",
    "    'grad_clip' : 5,\n",
    "    \"manualSeed\" : 1111, \"PAD\" : True ,'data_filtering_off' : True,'rgb' :True,'sensitive' : True, 'FT' : True,\n",
    "    'num_fiducial' : 20,\n",
    "    })\n",
    "converter = utils.AttnLabelConverter(opt.character)\n",
    "opt.num_classes = len(converter.character)\n",
    "device = torch.device('cuda') #utils.py 안에 device는 따로 세팅해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:06<00:00, 16468.00it/s]\n"
     ]
    }
   ],
   "source": [
    "ko_dataset = ko_dataset.dataset(num_samples = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOREAN SYNTHETIC\n",
    "kor_path = '/Data/FoodDetection/data/text_recognition/Korean/synthetic_data/data'\n",
    "\n",
    "kor_images_labels = []\n",
    "with open(os.path.join(kor_path, 'gt.txt'), 'r') as f:\n",
    "    files = f.readlines()\n",
    "    for idx, file in enumerate(files):\n",
    "        img_path, label = file.split(' ')\n",
    "        label = j2hcj(h2j(label.strip('\\n')))\n",
    "        img = Image.open(os.path.join(kor_path, f'{img_path}.jpg'))\n",
    "        kor_images_labels.append([img, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot identify image file '/Data/FoodDetection/data/text_recognition/English/synthetic_oxford2/77_heretical_35885.jpg'\n"
     ]
    }
   ],
   "source": [
    "# ENGLISH STR\n",
    "\n",
    "# eng_dataset = en_dataset.get_english_dataset(opt) # 얘는 일단보류\n",
    "eng_path = '/Data/FoodDetection/data/text_recognition/English/synthetic_oxford2'\n",
    "file_list = os.listdir(eng_path)\n",
    "eng_images_labels = []\n",
    "for file in file_list[:300000]:\n",
    "    try :\n",
    "        img = Image.open(os.path.join(eng_path, file))\n",
    "        label = file.split('_')[1].strip('\\n')\n",
    "        eng_images_labels.append([img, label])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_dataset.dataset.extend(kor_images_labels)\n",
    "ko_dataset.dataset.extend(eng_images_labels)\n",
    "random.shuffle(ko_dataset.dataset)\n",
    "len(ko_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = get_transform()\n",
    "dataset_streamer = utils.Dataset_streamer(ko_dataset.dataset[ : int(len(ko_dataset.dataset)*0.98)])\n",
    "valid_streamer = utils.Dataset_streamer(ko_dataset.dataset[int(len(ko_dataset.dataset)*0.98) : ])\n",
    "\n",
    "_AlignCollate = utils.AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=False)\n",
    "data_loader = DataLoader(dataset_streamer, batch_size = opt.batch_size,  num_workers =0, shuffle=True, #worker_init_fn=worker_init_fn, \n",
    "                         collate_fn = _AlignCollate, pin_memory=False )\n",
    "data_loader_iter = iter(data_loader)\n",
    "\n",
    "#for valid\n",
    "_AlignCollate_valid = utils.AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=True)\n",
    "#not valid_streamer\n",
    "valid_loader = DataLoader(valid_streamer, batch_size = opt.batch_size,  num_workers=0, shuffle=True, #worker_init_fn = worker_init_fn,\n",
    "                          collate_fn=_AlignCollate_valid, pin_memory=False)\n",
    "valid_loader_iter = iter(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(data_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx :  62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'ㅎㅏㄱㅂㅜㅁㅗ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_idx = np.random.choice(range(opt.batch_size), size=1)[0]\n",
    "plt.imshow(img[random_idx].numpy().transpose().swapaxes(1,0))\n",
    "print('idx : ',random_idx)\n",
    "plt.xlabel(label[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, length = converter.encode(label, batch_max_length = opt.batch_max_length)\n",
    "img_tensor = img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCATTER(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(SCATTER, self).__init__()\n",
    "        self.opt = opt\n",
    "        \n",
    "        #Trans\n",
    "        self.Trans = Trans.TPS_SpatialTransformerNetwork(F = opt.num_fiducial, i_size = (opt.imgH, opt.imgW), \n",
    "                                                  i_r_size= (opt.imgH, opt.imgW), i_channel_num=opt.input_channel, device = device)\n",
    "        \n",
    "        #Extract\n",
    "        self.Extract = Extract.RCNN_extractor(opt.input_channel, opt.output_channel)\n",
    "#         self.Extract = Extract.ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        self.FeatureExtraction_output = opt.output_channel # (imgH/16 -1 )* 512\n",
    "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None,1)) # imgH/16-1   ->  1\n",
    "            \n",
    "        # VISUAL FEATURES \n",
    "        self.VFR = VFR.Visual_Features_Refinement(kernel_size = (3,1), num_classes = opt.num_classes, \n",
    "                                              in_channels = self.FeatureExtraction_output, out_channels=1, stride=1)\n",
    "        \n",
    "        \n",
    "        # CTC DECODER\n",
    "        self.CTC = CTC.CTC_decoder(opt.output_channel, opt.output_channel, opt.num_classes, opt, device)\n",
    "            \n",
    "        # Selective Contextual Refinement Block\n",
    "        self.SCR_1 = SCR.Selective_Contextual_refinement_block(input_size = self.FeatureExtraction_output, \n",
    "                                                         hidden_size = int(self.FeatureExtraction_output/2),\n",
    "                                                        output_size = self.FeatureExtraction_output,\n",
    "                                                        num_classes = opt.num_classes, decoder_fix = False, device = device)\n",
    "        \n",
    "        self.SCR_2 = SCR.Selective_Contextual_refinement_block(input_size = self.FeatureExtraction_output, \n",
    "                                                         hidden_size = int(self.FeatureExtraction_output/2),\n",
    "                                                        output_size = self.FeatureExtraction_output,\n",
    "                                                        num_classes = opt.num_classes, decoder_fix = True, device = device)\n",
    "  \n",
    "    def forward(self, input, text, is_train=True):\n",
    "        # Trans stage\n",
    "        input = self.Trans(input)\n",
    "        \n",
    "        # Extract stage\n",
    "        visual_feature = self.Extract(input) # visual_feature.shape) # (192, 512, 1 , 23)\n",
    "        \n",
    "        # Visual Feature Refinement\n",
    "        visual_refined = self.VFR(visual_feature) # visual_ refined output Size([192, 23, 512])\n",
    "        \n",
    "        # CTC DECODER\n",
    "        ctc_prob  = self.CTC(visual_refined, text, opt)\n",
    "        \n",
    "        #Selective Contextual Refinement\n",
    "        scr_probs_1, H = self.SCR_1(visual_feature.permute(0, 3, 1, 2).squeeze(3), text, is_train)\n",
    "        scr_probs_2, _ = self.SCR_2(H, text, is_train)\n",
    "        \n",
    "        return [scr_probs_1, scr_probs_2] ,ctc_prob\n",
    "#         return [scr_probs_1, scr_probs_2] ,_\n",
    "#         return _, ctc_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter = SCATTER(opt)\n",
    "# scatter = scatter.cuda()\n",
    "# scr_probs, ctc_prob = scatter(img_tensor, text[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scatter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-f40735a7f268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scatter' is not defined"
     ]
    }
   ],
   "source": [
    "scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(scr_probs))\n",
    "# print(ctc_prob.shape)\n",
    "# ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 23, 143])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 27])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0248,  0.0175, -0.0008,  ..., -0.0223,  0.0072, -0.0080],\n",
       "        [ 0.0327,  0.0193, -0.0105,  ..., -0.0261, -0.0098, -0.0101],\n",
       "        [-0.0112, -0.0440, -0.0057,  ...,  0.0378,  0.0394,  0.0340],\n",
       "        ...,\n",
       "        [ 0.0178, -0.0274, -0.0108,  ..., -0.0225,  0.0356,  0.0108],\n",
       "        [ 0.0373, -0.0378,  0.0239,  ...,  0.0003, -0.0379,  0.0314],\n",
       "        [ 0.0279, -0.0012,  0.0331,  ...,  0.0197, -0.0165, -0.0302]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scatter.CTC.lstm.weight_ih_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0248,  0.0175, -0.0008,  ..., -0.0223,  0.0072, -0.0080],\n",
       "        [ 0.0327,  0.0193, -0.0105,  ..., -0.0261, -0.0098, -0.0101],\n",
       "        [-0.0112, -0.0440, -0.0057,  ...,  0.0378,  0.0394,  0.0340],\n",
       "        ...,\n",
       "        [ 0.0178, -0.0273, -0.0108,  ..., -0.0225,  0.0356,  0.0108],\n",
       "        [ 0.0373, -0.0378,  0.0239,  ...,  0.0003, -0.0379,  0.0314],\n",
       "        [ 0.0279, -0.0012,  0.0331,  ...,  0.0197, -0.0165, -0.0302]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scatter.CTC.lstm.weight_ih_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_criterion = torch.nn.CTCLoss(blank= 0, reduction = 'mean', zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_lengths = torch.full(size = (ctc_prob.size(0),), fill_value= ctc_prob.size(1), dtype=torch.long)\n",
    "output_lengths = torch.randint(low = 1, high = ctc_prob.size(1), size = (ctc_prob.size(0), ), dtype = torch.long)\n",
    "ctc_loss = ctc_criterion(ctc_prob.transpose(0,1), text[:, 1:], input_lengths, output_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6393, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctc_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params :  5804167\n"
     ]
    }
   ],
   "source": [
    "filtered_params = []\n",
    "params_num = []\n",
    "for p in filter(lambda p : p.requires_grad, scatter.parameters()):\n",
    "    filtered_params.append(p)\n",
    "    params_num.append(np.prod(p.size()))\n",
    "print('Trainable params : ', sum(params_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta(filtered_params, lr= opt.lr, rho = opt.rho, eps = opt.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opt):\n",
    "    \n",
    "    model = SCATTER(opt)\n",
    "    \n",
    "    # WEIGHT INITIALIZATION\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if 'localization_fc2' in name:\n",
    "#             print(f'Skip {name} as it is already initializaed')\n",
    "#             continue\n",
    "            \n",
    "#         try:\n",
    "#             if 'bias' in name:\n",
    "#                 init.constant_(param, 0.0)\n",
    "#             elif 'weight' in name:\n",
    "#                 init.kaiming_normal_(parm)\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             if 'weight' in name:\n",
    "#                 param.data.fill_(1)\n",
    "#             continue\n",
    "    \n",
    "#     print('layer SCR_1 initial weight :', model.SCR_1.BiLSTM_1.weight_ih_l0)\n",
    "#     print('layer SCR_1 initial grad :', model.SCR_1.BiLSTM_1.weight_ih_l0.grad)\n",
    "    \n",
    "    # DATA PARALLEL\n",
    "    model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)\n",
    "#     model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # CHECK PRETRAINED MODEL\n",
    "    if opt.saved_model!='':\n",
    "        base_path = './models'\n",
    "        try:\n",
    "            if opt.FT:\n",
    "                model.load_state_dict(torch.load(os.path.join(base_path, opt.saved_model)), strict=False)\n",
    "            else :\n",
    "                model.load_state_dict(torch.load(os.path.join(base_path, opt.saved_model)))\n",
    "            print('got your model!')\n",
    "        except Exception as e:\n",
    "            print(f'Coudnt load model. error is {e}')\n",
    "    \n",
    "    # LOSS \n",
    "    scr_criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "    ctc_criterion = torch.nn.CTCLoss(blank= 0, reduction = 'mean', zero_infinity=True).to(device)\n",
    "    \n",
    "    # Filter only require gradient descent\n",
    "    filtered_params = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p : p.requires_grad, model.parameters()):\n",
    "        filtered_params.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Trainable params : ', sum(params_num))\n",
    "    \n",
    "    # OPTIMIZER\n",
    "    optimizer = optim.Adadelta(filtered_params, lr= opt.lr, rho = opt.rho, eps = opt.eps)\n",
    "\n",
    "    # OPT LOG\n",
    "    with open(f'./models/{opt.experiment_name}/opt.txt', 'a') as opt_file:\n",
    "        opt_log = '-----------------Options--------------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log +=f'{str(k)} : {str(v)}\\n'\n",
    "        opt_log+= '--------------------------------------------\\n' \n",
    "        opt_file.write(opt_log)\n",
    "        \n",
    "    # START TRAIN\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    \n",
    "    for n_epoch, epoch in enumerate(range(opt.num_epoch)):\n",
    "        for n_iter, data_point in enumerate(data_loader):\n",
    "            images, labels = data_point\n",
    "            images = images.to(device)\n",
    "            text, length = converter.encode(labels, batch_max_length = opt.batch_max_length)\n",
    "            \n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            scr_probs, ctc_prob = model(images, text[:, 1:])\n",
    "            target = text[:, 1:]\n",
    "\n",
    "            losses = []\n",
    "#             input_lengths = torch.full(size = (ctc_prob.size(0),), fill_value= ctc_prob.size(1), dtype=torch.long)\n",
    "#             output_lengths = torch.randint(low = 1, high = ctc_prob.size(1), size = (ctc_prob.size(0), ), dtype = torch.long)\n",
    "#             ctc_loss = ctc_criterion(ctc_prob.transpose(0,1), text[:, 1:], input_lengths, output_lengths)\n",
    "            \n",
    "#             losses.append(opt.ctc_loss_lambda * ctc_loss)\n",
    "            for prob in scr_probs:\n",
    "                scr_loss = scr_criterion(prob.view(-1, prob.shape[-1]), target.contiguous().view(-1))\n",
    "                losses.append(opt.scr_loss_lambda * scr_loss)\n",
    "            \n",
    "            loss=0\n",
    "            loss_avg = utils.Averager()\n",
    "            for loss_ in losses:\n",
    "                loss+=loss_\n",
    "                loss_avg.add(loss_)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #VALIDATION\n",
    "            if n_iter % opt.valinterval ==0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                with open(f'./models/{opt.experiment_name}/log_train.txt' , 'a') as log:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():  #criterion arguments = ctc_criterion first\n",
    "                        valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels, infer_time, length_of_data = scatter_utils.validation(model, \n",
    "                        [ctc_criterion, scr_criterion], valid_loader, converter, opt)  \n",
    "                    model.train()\n",
    "                        \n",
    "                    present_time = time.localtime()\n",
    "                    loss_log = f'[epoch : {n_epoch}/{opt.num_epoch}] [iter : {n_iter*opt.batch_size} / {int(len(ko_dataset.dataset)*0.98)}]\\n'+\\\n",
    "                    f'Train loss : {loss_avg.val():0.5f}, Valid loss : {valid_loss:0.5f}, Elapsed time : {elapsed_time:0.5f}, Present time : {present_time[1]}/{present_time[2]}, {present_time[3]+9} : {present_time[4]}'\n",
    "                    loss_avg.reset()\n",
    "                    \n",
    "                    current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"current_norm_ED\":17s}: {current_norm_ED:0.2f}'\n",
    "                    \n",
    "                    #keep the best\n",
    "                    if current_accuracy > best_accuracy:\n",
    "                        best_accuracy = current_accuracy\n",
    "                        torch.save(model.state_dict(), f'./models/{opt.experiment_name}/best_accuracy.pth')\n",
    "                        \n",
    "                    if current_norm_ED > best_norm_ED:\n",
    "                        best_norm_ED = current_norm_ED\n",
    "                        torch.save(model.state_dict(), f'./models/{opt.experiment_name}/best_norm_ED.pth')\n",
    "                    \n",
    "                    best_model_log = f'{\"Best accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.2f}'\n",
    "                    loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                    print(loss_model_log)\n",
    "                    log.write(loss_model_log+'\\n')\n",
    "                    \n",
    "                    dashed_line = '-'*80\n",
    "                    head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s}|Confidence Score & T/F'\n",
    "                    predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "                    \n",
    "                    for gt, pred, confidence in zip(list(np.asarray(labels)[:5]), list(np.asarray(preds)[:5]), list(np.asarray(confidence_score)[:5])):\n",
    "        \n",
    "                        gt = gt[: gt.find('[s]')]\n",
    "                        pred = pred[: pred.find('[s]')]\n",
    "                        \n",
    "                        predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                    predicted_result_log += f'{dashed_line}'\n",
    "                    print(predicted_result_log)\n",
    "                    log.write(predicted_result_log+'\\n')\n",
    "        \n",
    "        if n_epoch % 10 ==0:\n",
    "            torch.save(model.state_dict(), f'./models/{opt.experiment_name}/{n_epoch}.pth')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Use multi GPU setting-------\n",
      "Trainable params :  37193125\n",
      "[epoch : 0/300] [iter : 0 / 587999]\n",
      "Train loss : 4.94733, Valid loss : 4.95421, Elapsed time : 0.87132, Present time : 6/8, 19 : 5\n",
      "Current_accuracy : 0.000, current_norm_ED  : 0.01\n",
      "Best accuracy    : 0.000, Best_norm_ED     : 0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction               |Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "ㅂㅏㄹㅈㅓㄴㄷㅚㄷㅏ                | eeeeeeeeeeeeeeeeeeeeeeeee | 0.0000\tFalse\n",
      "ㅇㅠㅎㅏㄱ                     | eeeeeeeeeeeeeeeeeeeeeeeee | 0.0000\tFalse\n",
      "ㅇㅓㄹㅕㅇㅝㅈㅣㄷㅏ                | eeeeeeeeeeeeeeeeeeeeeeeee | 0.0000\tFalse\n",
      "admonishing               | eeeeeeeeeeeeeeeeeeeeeeeee | 0.0000\tFalse\n",
      "ㅅㅣㄹㅁㅏㅇ                    | eeeeeeeeeeeeeeeeeeeeeeeee | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f'./models/{opt.experiment_name}', exist_ok=True)\n",
    "\n",
    "# set seed\n",
    "random.seed(opt.manualSeed)\n",
    "np.random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "torch.cuda.manual_seed(opt.manualSeed)\n",
    "\n",
    "# set GPU\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "opt.num_gpu = torch.cuda.device_count()\n",
    "\n",
    "if opt.num_gpu > 1:\n",
    "    print('-------Use multi GPU setting-------')\n",
    "    opt.batch_size = opt.batch_size * opt.num_gpu\n",
    "train(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
